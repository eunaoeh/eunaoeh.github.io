<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-08-18T02:39:05+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Eunaoeh’s Dev Blog</title><subtitle>...</subtitle><author><name>Eunjin</name></author><entry><title type="html">[논문리뷰] Vision Transformer(ViT)</title><link href="http://localhost:4000/paper%20review/cv/ViT/" rel="alternate" type="text/html" title="[논문리뷰] Vision Transformer(ViT)" /><published>2021-08-17T00:00:00+09:00</published><updated>2021-08-17T00:00:00+09:00</updated><id>http://localhost:4000/paper%20review/cv/ViT</id><content type="html" xml:base="http://localhost:4000/paper%20review/cv/ViT/">&lt;p&gt;논문: &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot;&gt;“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;핵심-요약&quot;&gt;핵심 요약&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Image Recognition에 Transformer 모델을 적용&lt;/li&gt;
  &lt;li&gt;컴퓨터비전에서 CNN의 구조를 사용한 모델의 한계점을 극복&lt;/li&gt;
  &lt;li&gt;image patch를 linear embedding하여 트랜스포머 모델에 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Vision Transformer(ViT)란 자연어처리(NLP) 분야에서 사실상 메인 모델이라고 할 수 있는 트랜스포머를 Image Classification 에 적용한 모델이다. 2021 ICLR에 구글 리서치 팀에서 발표한 논문으로,비전 분야에서도 CNN을 사용하지 않고 트랜스포머를 사용하여 좋은 성능을 낼 수 있다는 것이 핵심입니다. 컴퓨터비전에서는 보통 CNN을 기반으로 한 모델 구조를 사용하는데, 이번 논문에서는 CNN 구조를 활용하지 않아도 Image Classification에서 좋은 결과를 얻을 수 있었다고 합니다. ViT에서는 이미지를 패치로 잘라서 Vision Transfomer 모델에 학습을 시켰고, SOTA CNN 모델과 비교하였을 때 보다 적은 리소스로 좋은 성능을 냈다고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;
&lt;h3 id=&quot;1-vision-transformervit&quot;&gt;1. Vision Transformer(ViT)&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/2021-08-17-ViT/vit.png&quot; alt=&quot;vit&quot; /&gt; 
[출처] “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”
&lt;br /&gt;&lt;br /&gt;
모델의 구조는 트랜스포머 모델을 거의 똑같이 디자인하였습니다. 고정된 크기의 patch로 잘라진 이미지를 순서대로 embedding하여 인코더의 입력값으로 넣습니다. classification을 하기위해 “classification token”을 새로 추가하여 학습을 하였습니다.&lt;/p&gt;

&lt;h3 id=&quot;2-fine-tuning-and-higher-resolution&quot;&gt;2. Fine-tuning and Higher Resolution&lt;/h3&gt;
&lt;p&gt;ViT를 큰 데이터셋에 pre-train한 후, 작은 downstream tasks에 fine-tuning 합니다.&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/2021-08-17-ViT/ViT-exp.png&quot; alt=&quot;ViT-exp&quot; /&gt; 
[출처] “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;h2 id=&quot;출처&quot;&gt;출처&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot;&gt;“An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
틀린 부분이나 부족한 점이 있다면 알려주시면 감사하겠습니다:)&lt;/p&gt;</content><author><name>Eunjin</name></author><category term="Paper Review" /><category term="CV" /><category term="CV" /><category term="Image Classification" /><category term="Paper Review" /><category term="DL" /><summary type="html">논문: “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” 핵심 요약 Image Recognition에 Transformer 모델을 적용 컴퓨터비전에서 CNN의 구조를 사용한 모델의 한계점을 극복 image patch를 linear embedding하여 트랜스포머 모델에 학습</summary></entry></feed>