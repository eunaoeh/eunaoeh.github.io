---
title: "[논문리뷰] Vision Transformer(ViT)"

categories:
  - Paper Review
  - CV

tags:
  - CV
  - Image Classification
  - Paper Review
  - DL
  
use_math: true
toc: true
toc_sticky: true
---

<br>
< 현재 페이지는 아직 작성중입니다. >
<br><br>
논문: ["An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"](https://arxiv.org/pdf/2010.11929.pdf)
## 핵심 요약
- Image Recognition에 NLP에서 사용되던 모델 Transformer를 적용
- 컴퓨터비전에서 CNN의 구조를 사용한 모델 구조를 대체
- Image patch를 linear embedding하여 트랜스포머와 거의 같은 구조의 모델에 학습
- CNN 기반의 SOTA 모델보다 좋은 성능
  - 단, 큰 데이터셋으로 pre-trained 후 fine-tuning이 필요

## Abstract
Vision Transformer(ViT)란 자연어처리(NLP) 분야에서 사실상 메인 모델이라고 할 수 있는 트랜스포머를 Image Classification 에 적용한 모델입니다. 2021 ICLR에 구글 리서치 팀에서 발표한 논문으로,비전 분야에서도 CNN을 사용하지 않고 트랜스포머를 사용하여 좋은 성능을 낼 수 있다는 것이 핵심입니다. 컴퓨터비전에서는 보통 CNN을 기반으로 한 모델 구조를 사용하는데, 이번 논문에서는 CNN 구조를 활용하지 않아도 Image Classification에서 좋은 결과를 얻을 수 있다는 것을 보여줍니다. ViT에서는 이미지를 패치로 잘라서 Vision Transfomer 모델에 학습을 시켰고, SOTA CNN 모델과 비교하였을 때 보다 적은 리소스로 좋은 성능을 냈습니다.

## Method
### 1. Vision Transformer(ViT)
![vit](/assets/images/2021-08-17-ViT/vit.png) 
[출처] "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
<br>
<br>
모델의 구조는 트랜스포머 모델을 거의 똑같이 디자인하였습니다. 고정된 크기의 patch로 잘라진 이미지를 순서대로 embedding하여 인코더의 입력값으로 넣습니다. 
이미지의 차원이 $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$이고, patch의 해상도가 (P, P)라고 하면 $\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$ 으로 flatten시킵니다. 여기서 트랜스포머는 인코더에서 정해진 입력의 크기를 사용하기 때문에 이를 맞춰주기 위해 입력 차원을 D 차원을 linear projection을 해줍니다. 이 linear project의 결과 값에 positional embedding을 추가해준 값이 ViT의 인코더의 입력값으로 들어가게 됩니다. 
<!-- classification을 하기위해 "classification token"을 새로 추가하여 학습을 하였습니다.  -->

Positinal Embedding은 이미지의 공간적인 정보를 위해 1D learnable embeddings을 추가하여 학습시켰습니다. 

#### Inductive Bias
[Inductive Bias](https://velog.io/@euisuk-chung/Inductive-Bias%EB%9E%80)에 대한 설명은 링크에 잘 나와있습니다.
ViT에서는 MLP 레이어에서만 Locality와 Translation Equivariance한 특징을 가지고 있고, Self-Attention Layer에서는 Global한 이미지의 feature를 학습하게 됩니다. 아마, Transformer 모델의 구조를 이해하셨다면 쉽게 이해가 되실 겁니다. 즉, 다른 이미지 패치와의 Attention을 학습하기 때문에 Global한 feature를 학습한다고 할 수 있습니다.
Fine-tuning시에는 다른 해상도의 이미지를 사용하는 경우, 패치의 2D 공간에 대한 정보를 가지고 있지 않기 때문에 positional embedding을 처음부터 학습해야합나다. 

### 2. Fine-tuning and Higher Resolution
ViT를 큰 데이터셋에 pre-train한 후, 작은 downstream tasks에 fine-tuning 합니다. 


## Experiment
![ViT-exp](/assets/images/2021-08-17-ViT/ViT-exp.png) 
[출처] "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
<br>
<br>

## Conclusion

## 출처
- ["An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"](https://arxiv.org/pdf/2010.11929.pdf)

<br><br>
틀린 부분이나 부족한 점이 있다면 알려주시면 감사하겠습니다:)